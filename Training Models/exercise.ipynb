{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Softmax regression without using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import label_binarize\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = label_binarize(iris['target'], classes = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self):\n",
    "        #weight matrix, shape = (input, output)\n",
    "        self.W = None\n",
    "\n",
    "    def softmax(self, scores):\n",
    "        return np.exp(scores) / np.expand_dims(np.sum(np.exp(scores), axis = 1), axis = -1)\n",
    "\n",
    "    def learning_schedule(self, eta_):\n",
    "        return eta_ * 0.95\n",
    "        \n",
    "    def fit(self, X, y, warm_start = False, epochs = 1000, eta = 0.01, batch_size = 4):\n",
    "        if len(X.shape) < 2 or len(y.shape) < 2:\n",
    "            print('Check the shapes of the input data!')\n",
    "            raise ValueError\n",
    "        elif X.shape[0] != y.shape[0]:\n",
    "            print('You have entered the wrong number of samples for the data and targets!')\n",
    "            raise ValueError\n",
    "        X = np.c_[X, np.ones(shape = (X.shape[0], 1))] #add the bias to the input\n",
    "        \n",
    "        if not warm_start:\n",
    "            self.W = np.random.randn(X.shape[1], y.shape[1])\n",
    "        elif warm_start and type(self.W) == type(None):\n",
    "            print('Cannot use warm start!', '\\n Pass some data so that the model can initialize the weights!')\n",
    "        \n",
    "        m = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_indexes = list(range(0, m, batch_size))\n",
    "            if batch_indexes[-1] != m-1:\n",
    "                batch_indexes.append(m-1)\n",
    "\n",
    "            for index in range(len(batch_indexes) - 1):\n",
    "                X_batch = X[batch_indexes[index]:batch_indexes[index+1], :]\n",
    "                y_batch = y[batch_indexes[index]:batch_indexes[index+1], :]\n",
    "                current_batch_size = batch_indexes[index + 1] - batch_indexes[index]\n",
    "                \n",
    "                scores = X_batch.dot(self.W)\n",
    "                probability = self.softmax(scores)\n",
    "\n",
    "                cost = 0\n",
    "                for i in range(current_batch_size):\n",
    "                    cost += np.sum(y_batch[i] * np.log(probability[i]))\n",
    "                cost = -1/current_batch_size * cost\n",
    "\n",
    "                gradient = np.zeros(shape = self.W.shape)\n",
    "                for i in range(current_batch_size):\n",
    "                    gradient += np.expand_dims(X_batch[i], axis = -1) @ np.expand_dims(probability[i] - y_batch[i], axis = -1).T\n",
    "                gradient = 1/current_batch_size * gradient\n",
    "                self.W = self.W - eta * gradient\n",
    "            \n",
    "            eta = self.learning_schedule(eta)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) < 2:\n",
    "            X = np.expand_dims(X, axis = -1).T\n",
    "        X = np.c_[X, np.ones(shape = (X.shape[0], 1))]\n",
    "        scores = X.dot(self.W)\n",
    "        probability = self.softmax(scores)\n",
    "        return probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "softmax_reg = SoftmaxRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "type(softmax_reg.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.fit(X_train, y_train, epochs = 1000, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = softmax_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_sparse = np.argmax(y_pred, axis = 1)\n",
    "y_test_sparse = np.argmax(y_test, axis = 1)\n",
    "accuracy_score(y_test_sparse, y_pred_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
